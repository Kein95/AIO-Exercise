# -*- coding: utf-8 -*-
"""M6_W4_3_Text_Classification_using_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aym1jhXia38RRGbCtoaB3pqTa9Z3UtpI
"""

!pip install transformers datasets

# Tải bộ dữ liệu từ HuggingFace Datasets
from datasets import load_dataset

# Load dataset NTC-SCV
ds = load_dataset('thainq107/ntc-scv')

from transformers import AutoTokenizer

# Tên mô hình BERT
model_name = "distilbert-base-uncased"  # Hoặc "bert-base-uncased"

# Khởi tạo tokenizer từ mô hình pretrained
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True  # Sử dụng tokenizer nhanh hơn
)

# Độ dài tối đa của chuỗi
max_seq_length = 100
max_seq_length = min(max_seq_length, tokenizer.model_max_length)

# Hàm xử lý dữ liệu
def preprocess_function(examples):
    # Tokenize các văn bản
    result = tokenizer(
        examples["preprocessed_sentence"],  # Cột chứa văn bản đã tiền xử lý
        padding="max_length",  # Đệm chuỗi đến max_seq_length
        max_length=max_seq_length,
        truncation=True  # Cắt bớt chuỗi nếu dài hơn max_seq_length
    )
    result["label"] = examples['label']  # Gán nhãn tương ứng
    return result

# Tiền xử lý tập dữ liệu bằng tokenizer
processed_dataset = ds.map(
    preprocess_function,
    batched=True,  # Xử lý theo batch để tăng tốc độ
    desc="Running tokenizer on dataset",
)

from transformers import AutoConfig, AutoModelForSequenceClassification

# Số nhãn trong bài toán phân loại
num_labels = 2

# Cấu hình mô hình
config = AutoConfig.from_pretrained(
    model_name,  # Tên mô hình đã được định nghĩa trước đó (ví dụ: "distilbert-base-uncased")
    num_labels=num_labels,  # Số nhãn trong bài toán phân loại
    finetuning_task="text-classification"  # Nhiệm vụ fine-tuning
)

# Khởi tạo mô hình BERT cho bài toán phân loại
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    config=config  # Sử dụng cấu hình đã định nghĩa
)

!pip install evaluate

import numpy as np
import evaluate

# Tải metric "accuracy" từ thư viện evaluate
metric = evaluate.load("accuracy")

# Hàm tính toán các chỉ số đánh giá
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Lấy nhãn dự đoán có xác suất cao nhất
    predictions = np.argmax(predictions, axis=1)
    # Tính toán độ chính xác
    result = metric.compute(predictions=predictions, references=labels)
    return result

# Chia tập train thành train và validation (80:20)
split_dataset = processed_dataset["train"].train_test_split(test_size=0.2)

# Thêm vào DatasetDict
processed_dataset = {
    "train": split_dataset["train"],
    "validation": split_dataset["test"],
    "test": processed_dataset["test"]
}

# Chuyển đổi thành DatasetDict
from datasets import DatasetDict
processed_dataset = DatasetDict(processed_dataset)

# Kiểm tra kết quả
print(processed_dataset)

from transformers import TrainingArguments, Trainer

# Cấu hình tham số huấn luyện
training_args = TrainingArguments(
    output_dir="save_model",  # Thư mục lưu mô hình
    learning_rate=2e-5,  # Tốc độ học
    per_device_train_batch_size=128,  # Kích thước batch cho tập train
    per_device_eval_batch_size=128,  # Kích thước batch cho tập eval
    num_train_epochs=10,  # Số epoch
    eval_strategy="epoch",  # Đánh giá trên tập validation mỗi epoch
    save_strategy="epoch",  # Lưu mô hình tốt nhất mỗi epoch
    load_best_model_at_end=True  # Tải mô hình tốt nhất sau khi huấn luyện
)

# Tạo Trainer
trainer = Trainer(
    model=model,  # Mô hình đã được khởi tạo
    args=training_args,  # Tham số huấn luyện
    train_dataset=processed_dataset["train"],  # Dữ liệu huấn luyện
    eval_dataset=processed_dataset["validation"],  # Dữ liệu validation
    compute_metrics=compute_metrics,  # Hàm tính toán chỉ số đánh giá
    tokenizer=tokenizer,  # Tokenizer
)

# Huấn luyện mô hình
trainer.train()