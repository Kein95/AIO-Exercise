# -*- coding: utf-8 -*-
"""M6_W1_3_NTC_SCV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gUd_Ac267xH_wfMCZSFWV_VhGDgnIIWX
"""

# Clone and extract the dataset
!git clone https://github.com/congnghia0609/ntc-scv.git
!unzip ./ntc-scv/data/data_test.zip -d ./data
!unzip ./ntc-scv/data/data_train.zip -d ./data
!rm -rf ./ntc-scv

# Load data from paths to dataframe
import os
import pandas as pd

def load_data_from_path(folder_path):
    examples = []
    for label in os.listdir(folder_path):
        full_path = os.path.join(folder_path, label)
        for file_name in os.listdir(full_path):
            file_path = os.path.join(full_path, file_name)
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
                sentence = " ".join(lines).strip()  # Combine lines and remove extra spaces
            if label == "neg":
                label = 0
            elif label == "pos":
                label = 1
            data = {
                'sentence': sentence,
                'label': label
            }
            examples.append(data)
    return pd.DataFrame(examples)

# Define folder paths
folder_paths = {
    'train': './data/data_train/train',
    'valid': './data/data_train/test',
    'test': './data/data_test/test'
}

# Load data into dataframes
train_df = load_data_from_path(folder_paths['train'])
valid_df = load_data_from_path(folder_paths['valid'])
test_df = load_data_from_path(folder_paths['test'])

# Display a sample of the dataframes
print("Train Data Sample:")
print(train_df.head())
print("\nValidation Data Sample:")
print(valid_df.head())
print("\nTest Data Sample:")
print(test_df.head())

# Install langid library
!pip install langid

from langid.langid import LanguageIdentifier, model

def identify_vn(df):
    """
    Identify rows in the dataframe where the text is likely Vietnamese (vi).

    Args:
    df (pd.DataFrame): Input dataframe with a 'sentence' column.

    Returns:
    tuple: Two dataframes - one with Vietnamese rows and another with non-Vietnamese rows.
    """
    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)
    not_vi_idx = set()
    THRESHOLD = 0.9  # Confidence threshold for identifying Vietnamese

    for idx, row in df.iterrows():
        score = identifier.classify(row["sentence"])
        if score[0] != "vi" or (score[0] == "vi" and score[1] <= THRESHOLD):
            not_vi_idx.add(idx)

    vi_df = df[~df.index.isin(not_vi_idx)]  # Vietnamese rows
    not_vi_df = df[df.index.isin(not_vi_idx)]  # Non-Vietnamese rows

    return vi_df, not_vi_df

# Filter Vietnamese sentences in the training dataset
train_df_vi, train_df_other = identify_vn(train_df)

# Display samples
print("Vietnamese Rows (Sample):")
print(train_df_vi.head())
print("\nNon-Vietnamese Rows (Sample):")
print(train_df_other.head())

import re
import string

def preprocess_text(text):
    """
    Preprocess the input text by removing URLs, HTML tags, punctuation, digits, and emojis.
    Args:
        text (str): The input text string.
    Returns:
        str: The cleaned and lowercased text.
    """
    # Remove URLs
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    text = url_pattern.sub(r" ", text)

    # Remove HTML tags
    html_pattern = re.compile(r'<[^<>]+>')
    text = html_pattern.sub(" ", text)

    # Remove punctuation and digits
    replace_chars = string.punctuation + string.digits
    for char in replace_chars:
        text = text.replace(char, " ")

    # Remove emojis
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"  # other pictographs
        u"\U0001F1F2-\U0001F1F4"  # Macau flag
        u"\U0001F1E6-\U0001F1FF"  # flags
        u"\U000024C2-\U0001F251"  # enclosed characters
        u"\U0001f926-\U0001f937"  # gestures
        u"\U0001F1F2"  # additional flags
        u"\U0001F1F4"
        u"\U0001F620"
        u"\u200d"
        u"\u2640-\u2642"  # gender symbols
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub(r" ", text)

    # Remove extra spaces and convert to lowercase
    text = " ".join(text.split())

    return text.lower()

# Apply preprocessing to datasets
train_df_vi['preprocess_sentence'] = train_df_vi['sentence'].apply(preprocess_text)
valid_df['preprocess_sentence'] = valid_df['sentence'].apply(preprocess_text)
test_df['preprocess_sentence'] = test_df['sentence'].apply(preprocess_text)

# Install torchtext
!pip install -q torchtext==0.16.0

# Import necessary modules
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.data.functional import to_map_style_dataset

# Word-based tokenizer
tokenizer = get_tokenizer("basic_english")

# Function to yield tokens from sentences
def yield_tokens(sentences, tokenizer):
    for sentence in sentences:
        yield tokenizer(sentence)

# Build vocabulary
vocab_size = 10000  # Maximum vocabulary size
vocabulary = build_vocab_from_iterator(
    yield_tokens(train_df_vi['preprocess_sentence'], tokenizer),
    max_tokens=vocab_size,
    specials=["<pad>", "<unk>"]
)
vocabulary.set_default_index(vocabulary["<unk>"])

# Function to prepare dataset
def prepare_dataset(df):
    """
    Convert DataFrame into an iterable dataset for torchtext.

    Args:
    df (pd.DataFrame): Input DataFrame with preprocessed sentences and labels.

    Yields:
    tuple: (encoded_sentence, label)
    """
    for index, row in df.iterrows():
        sentence = row['preprocess_sentence']
        encoded_sentence = vocabulary(tokenizer(sentence))  # Tokenize and encode sentence
        label = row['label']
        yield encoded_sentence, label

# Prepare datasets
train_dataset = prepare_dataset(train_df_vi)
train_dataset = to_map_style_dataset(train_dataset)

valid_dataset = prepare_dataset(valid_df)
valid_dataset = to_map_style_dataset(valid_dataset)

import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence  # Import để hỗ trợ padding

# Define the collate function for batching
def collate_batch(batch):
    """
    Collates a batch of data by padding encoded sentences and converting labels to tensors.

    Args:
        batch (list of tuples): Each tuple contains (encoded_sentence, label).

    Returns:
        tuple: (padded_encoded_sentences, labels)
    """
    encoded_sentences, labels = [], []

    for encoded_sentence, label in batch:
        labels.append(label)
        encoded_sentence = torch.tensor(encoded_sentence, dtype=torch.int64)
        encoded_sentences.append(encoded_sentence)

    labels = torch.tensor(labels, dtype=torch.int64)
    encoded_sentences = pad_sequence(
        encoded_sentences,
        batch_first=True,  # Padding is applied along the last dimension
        padding_value=vocabulary["<pad>"]
    )

    return encoded_sentences, labels

# Define batch size
batch_size = 128

# Create DataLoaders
train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=collate_batch
)

valid_dataloader = DataLoader(
    valid_dataset,
    batch_size=batch_size,
    shuffle=False,
    collate_fn=collate_batch
)

import torch
import torch.nn as nn
import torch.nn.functional as F

class TextCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes):
        """
        TextCNN model for text classification.

        Args:
            vocab_size (int): Size of the vocabulary.
            embedding_dim (int): Dimension of word embeddings.
            kernel_sizes (list of int): Sizes of convolution kernels.
            num_filters (int): Number of filters for each kernel size.
            num_classes (int): Number of output classes.
        """
        super(TextCNN, self).__init__()

        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.kernel_sizes = kernel_sizes
        self.num_filters = num_filters
        self.num_classes = num_classes

        # Embedding layer with padding
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # Convolutional layers with multiple kernel sizes
        self.conv = nn.ModuleList([
            nn.Conv1d(
                in_channels=embedding_dim,
                out_channels=num_filters,
                kernel_size=k,
                stride=1
            ) for k in kernel_sizes
        ])

        # Fully connected layer
        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        """
        Forward pass for the TextCNN model.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length).

        Returns:
            torch.Tensor: Output logits of shape (batch_size, num_classes).
        """
        # Embedding lookup and transpose to (batch_size, embedding_dim, sequence_length)
        batch_size, sequence_length = x.shape
        x = self.embedding(x).transpose(1, 2)

        # Apply convolution and ReLU activation
        x = [F.relu(conv(x)) for conv in self.conv]

        # Apply max pooling over the sequence length dimension
        x = [F.max_pool1d(c, c.size(-1)).squeeze(dim=-1) for c in x]

        # Concatenate all feature maps
        x = torch.cat(x, dim=1)

        # Fully connected layer for classification
        x = self.fc(x)

        return x

import time
import torch

# Training function
def train(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):
    """
    Train the model for one epoch.

    Args:
        model (nn.Module): The model to be trained.
        optimizer (torch.optim.Optimizer): The optimizer for updating weights.
        criterion (nn.Module): The loss function.
        train_dataloader (DataLoader): DataLoader for the training data.
        device (torch.device): Device to run the training on.
        epoch (int): Current epoch number.
        log_interval (int): Number of batches after which to log training progress.

    Returns:
        tuple: (epoch_acc, epoch_loss) - accuracy and loss for the epoch.
    """
    model.train()
    total_acc, total_count = 0, 0
    losses = []
    start_time = time.time()

    for idx, (inputs, labels) in enumerate(train_dataloader):
        inputs = inputs.to(device)
        labels = labels.to(device)

        # Zero gradients
        optimizer.zero_grad()

        # Predictions
        predictions = model(inputs)

        # Compute loss
        loss = criterion(predictions, labels)
        losses.append(loss.item())

        # Backward pass and optimizer step
        loss.backward()
        optimizer.step()

        # Calculate accuracy
        total_acc += (predictions.argmax(1) == labels).sum().item()
        total_count += labels.size(0)

        # Log progress
        if idx % log_interval == 0 and idx > 0:
            elapsed = time.time() - start_time
            print(
                f"| epoch {epoch:3d} | {idx:5d}/{len(train_dataloader):5d} batches "
                f"| accuracy {total_acc / total_count:8.3f}"
            )
            total_acc, total_count = 0, 0
            start_time = time.time()

    epoch_acc = total_acc / total_count
    epoch_loss = sum(losses) / len(losses)
    return epoch_acc, epoch_loss


# Evaluation function
def evaluate(model, criterion, valid_dataloader, device):
    """
    Evaluate the model on validation data.

    Args:
        model (nn.Module): The model to be evaluated.
        criterion (nn.Module): The loss function.
        valid_dataloader (DataLoader): DataLoader for the validation data.
        device (torch.device): Device to run the evaluation on.

    Returns:
        tuple: (epoch_acc, epoch_loss) - accuracy and loss for the epoch.
    """
    model.eval()
    total_acc, total_count = 0, 0
    losses = []

    with torch.no_grad():
        for idx, (inputs, labels) in enumerate(valid_dataloader):
            inputs = inputs.to(device)
            labels = labels.to(device)

            # Predictions
            predictions = model(inputs)

            # Compute loss
            loss = criterion(predictions, labels)
            losses.append(loss.item())

            # Calculate accuracy
            total_acc += (predictions.argmax(1) == labels).sum().item()
            total_count += labels.size(0)

    epoch_acc = total_acc / total_count
    epoch_loss = sum(losses) / len(losses)
    return epoch_acc, epoch_loss

# Define model parameters
num_class = 2  # Number of output classes
vocab_size = len(vocabulary)  # Vocabulary size
embedding_dim = 100  # Embedding dimension
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Select device

# Initialize TextCNN model
model = TextCNN(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    kernel_sizes=[3, 4, 5],  # Kernel sizes for convolution layers
    num_filters=100,  # Number of filters for each kernel size
    num_classes=num_class
)
model.to(device)

# Define loss function and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Training configurations
num_epochs = 10
save_model = './model'
if not os.path.exists(save_model):
    os.makedirs(save_model)  # Ensure save directory exists

train_accs, train_losses = [], []
eval_accs, eval_losses = [], []
best_loss_eval = float('inf')  # Initialize with a large value

# Training and evaluation loop
for epoch in range(1, num_epochs + 1):
    epoch_start_time = time.time()

    # Training
    train_acc, train_loss = train(model, optimizer, criterion, train_dataloader, device, epoch)
    train_accs.append(train_acc)
    train_losses.append(train_loss)

    # Evaluation
    eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader, device)
    eval_accs.append(eval_acc)
    eval_losses.append(eval_loss)

    # Save the best model
    if eval_loss < best_loss_eval:
        torch.save(model.state_dict(), os.path.join(save_model, 'text_cnn_model.pt'))
        best_loss_eval = eval_loss

    # Print epoch summary
    print("-" * 59)
    print(
        f"| End of epoch {epoch:3d} | Time: {time.time() - epoch_start_time:5.2f}s | "
        f"Train Accuracy {train_acc:8.3f} | Train Loss {train_loss:8.3f} "
        f"| Valid Accuracy {eval_acc:8.3f} | Valid Loss {eval_loss:8.3f} "
    )
    print("-" * 59)

# Load the best model
model.load_state_dict(torch.load(os.path.join(save_model, 'text_cnn_model.pt')))
model.eval()

# Prepare test dataset
test_dataset = prepare_dataset(test_df)
test_dataset = to_map_style_dataset(test_dataset)

# Create DataLoader for test dataset
test_dataloader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    shuffle=False,  # No need to shuffle for test data
    collate_fn=collate_batch
)

# Evaluate the model on the test data
test_acc, test_loss = evaluate(model, criterion, test_dataloader, device)

# Print test accuracy and loss
print(f"Test Accuracy: {test_acc:.3f}")
print(f"Test Loss: {test_loss:.3f}")