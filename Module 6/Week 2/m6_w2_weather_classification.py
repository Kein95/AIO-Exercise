# -*- coding: utf-8 -*-
"""M6_W2_Weather_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18fQjD2yA8qK6ZjJtIdjWvmpYBlS9uc4u
"""

import torch
import torch.nn as nn
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

seed = 59
set_seed(seed)

from google.colab import drive
drive.mount('/content/drive')

!pip install gdown

import os

# Tải toàn bộ folder
!gdown --folder https://drive.google.com/drive/folders/14x4CCby6F3H4KybNpBp3pmBeYY3FXDjm -O /content/

# Giải nén tất cả file zip (nếu có)
for file in os.listdir('/content/'):
    if file.endswith(".zip"):
        os.system(f"unzip -o /content/{file} -d /content/data")

import os

# Đảm bảo đường dẫn tồn tại
zip_path = "/content/Data/img_cls_scenes_classification.zip"
extract_path = "/content/Data/scenes_classification"

# Tạo thư mục giải nén (nếu chưa có)
os.makedirs(extract_path, exist_ok=True)

# Giải nén file
!unzip -o "{zip_path}" -d "{extract_path}"

import os

# Đường dẫn đến file zip và thư mục đích
zip_path = "/content/Data/img_cls_weather_dataset.zip"
extract_path = "/content/Data/weather_dataset"

# Tạo thư mục giải nén (nếu chưa có)
os.makedirs(extract_path, exist_ok=True)

# Giải nén file zip
!unzip -o "{zip_path}" -d "{extract_path}"

pip install torch torchvision scikit-learn pillow matplotlib

root_dir = '/content/Data/weather_dataset/weather-dataset/dataset'
img_paths, labels = [], []

classes = {label_idx: class_name for label_idx, class_name in enumerate(sorted(os.listdir(root_dir)))}

for label_idx, class_name in classes.items():
    class_dir = os.path.join(root_dir, class_name)
    for img_filename in os.listdir(class_dir):
        img_path = os.path.join(class_dir, img_filename)
        img_paths.append(img_path)
        labels.append(label_idx)

img_paths = []
labels = []
for label_idx, class_name in classes.items():
    class_dir = os.path.join(root_dir, class_name)
    for img_filename in os.listdir(class_dir):
        img_path = os.path.join(class_dir, img_filename)
        img_paths.append(img_path)
        labels.append(label_idx)

val_size = 0.2
test_size = 0.125
is_shuffle = True

X_train, X_val, y_train, y_val = train_test_split(
    img_paths, labels,
    test_size=val_size,
    random_state=seed,
    shuffle=is_shuffle
)

X_train, X_test, y_train, y_test = train_test_split(
    X_train, y_train,
    test_size=test_size,
    random_state=seed,
    shuffle=is_shuffle
)

class WeatherDataset(Dataset):
    def __init__(
        self,
        X, y,
        transform=None
    ):
        self.transform = transform
        self.img_paths = X
        self.labels = y

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        img = Image.open(img_path).convert("RGB")

        if self.transform:
            img = self.transform(img)

        return img, self.labels[idx]

def transform(img, img_size=(224, 224)):
    img = img.resize(img_size)
    img = np.array(img)[..., :3]
    img = torch.tensor(img).permute(2, 0, 1).float()
    normalized_img = img / 255.0

    return normalized_img

train_dataset = WeatherDataset(
    X_train, y_train,
    transform=transform
)
val_dataset = WeatherDataset(
    X_val, y_val,
    transform=transform
)
test_dataset = WeatherDataset(
    X_test, y_test,
    transform=transform
)

train_batch_size = 512
test_batch_size = 8

train_loader = DataLoader(
    train_dataset,
    batch_size=train_batch_size,
    shuffle=True
)
val_loader = DataLoader(
    val_dataset,
    batch_size=test_batch_size,
    shuffle=False
)
test_loader = DataLoader(
    test_dataset,
    batch_size=test_batch_size,
    shuffle=False
)

import torch
print(torch.__version__)
print("CUDA Available:", torch.cuda.is_available())

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.batch_norm1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.batch_norm2 = nn.BatchNorm2d(out_channels)

        # Downsample shortcut connection nếu kích thước không khớp
        self.downsample = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

        self.relu = nn.ReLU()

    def forward(self, x):
        shortcut = x.clone()  # Shortcut ban đầu
        x = self.conv1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.batch_norm2(x)
        x += self.downsample(shortcut)  # Thêm shortcut vào đường chính
        x = self.relu(x)

        return x

class ResNet(nn.Module):
    def __init__(self, residual_block, n_blocks_lst, n_classes):
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.batch_norm1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.conv2 = self.create_layer(residual_block, 64, 64, n_blocks_lst[0], 1)
        self.conv3 = self.create_layer(residual_block, 64, 128, n_blocks_lst[1], 2)
        self.conv4 = self.create_layer(residual_block, 128, 256, n_blocks_lst[2], 2)
        self.conv5 = self.create_layer(residual_block, 256, 512, n_blocks_lst[3], 2)

        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(512, n_classes)

    def create_layer(self, residual_block, in_channels, out_channels, n_blocks, stride):
        blocks = []
        # Block đầu tiên trong layer
        first_block = residual_block(in_channels, out_channels, stride)
        blocks.append(first_block)

        # Các block tiếp theo trong layer
        for idx in range(1, n_blocks):
            block = residual_block(out_channels, out_channels, stride=1)
            blocks.append(block)

        # Gom các block vào một Sequential
        block_sequential = nn.Sequential(*blocks)
        return block_sequential

    def forward(self, x):
        x = self.conv1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)

        x = self.avgpool(x)
        x = self.flatten(x)
        x = self.fc1(x)

        return x

n_classes = len(list(classes.keys()))
device = 'cuda' if torch.cuda.is_available() else 'cpu'

model = ResNet(
    residual_block=ResidualBlock,
    n_blocks_lst=[2, 2, 2, 2],
    n_classes=n_classes
).to(device)

def evaluate(model, dataloader, criterion, device):
    model.eval()  # Đưa model vào chế độ đánh giá (evaluation mode)
    correct = 0
    total = 0
    losses = []

    with torch.no_grad():  # Tắt tính toán gradient để giảm bộ nhớ và tốc độ
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)  # Truyền dữ liệu qua mô hình
            loss = criterion(outputs, labels)  # Tính giá trị loss
            losses.append(loss.item())

            _, predicted = torch.max(outputs.data, 1)  # Dự đoán lớp có xác suất cao nhất
            total += labels.size(0)  # Tổng số mẫu dữ liệu
            correct += (predicted == labels).sum().item()  # Số lượng dự đoán đúng

    loss = sum(losses) / len(losses)  # Tính trung bình loss
    acc = correct / total  # Tính accuracy

    return loss, acc

def fit(
    model,
    train_loader,
    val_loader,
    criterion,
    optimizer,
    device,
    epochs
):
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        batch_train_losses = []

        model.train()  # Đặt model ở chế độ huấn luyện
        for idx, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()  # Đặt lại gradient về 0
            outputs = model(inputs)  # Truyền dữ liệu qua mô hình
            loss = criterion(outputs, labels)  # Tính giá trị loss
            loss.backward()  # Tính gradient
            optimizer.step()  # Cập nhật trọng số

            batch_train_losses.append(loss.item())  # Ghi nhận loss của batch

        # Tính loss trung bình của epoch trên tập train
        train_loss = sum(batch_train_losses) / len(batch_train_losses)
        train_losses.append(train_loss)

        # Đánh giá mô hình trên tập validation
        val_loss, val_acc = evaluate(
            model, val_loader,
            criterion, device
        )
        val_losses.append(val_loss)

        # In kết quả của epoch
        print(f'EPOCH {epoch + 1}:\tTrain loss: {train_loss:.4f}\tVal loss: {val_loss:.4f}')

    return train_losses, val_losses

lr = 1e-2  # Learning rate
epochs = 25  # Số epochs huấn luyện

criterion = nn.CrossEntropyLoss()  # Hàm loss cho bài toán phân loại
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=lr
)

train_losses, val_losses = fit(
    model,
    train_loader,
    val_loader,
    criterion,
    optimizer,
    device,
    epochs
)

print("Training Losses:", train_losses)
print("Validation Losses:", val_losses)

import matplotlib.pyplot as plt

epochs_range = range(1, epochs + 1)

plt.plot(epochs_range, train_losses, label="Train Loss")
plt.plot(epochs_range, val_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.show()

val_loss, val_acc = evaluate(
    model,
    val_loader,
    criterion,
    device
)
test_loss, test_acc = evaluate(
    model,
    test_loader,
    criterion,
    device
)

print('Evaluation on val/test dataset')
print('Val accuracy:', val_acc)
print('Test accuracy:', test_acc)