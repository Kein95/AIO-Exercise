# -*- coding: utf-8 -*-
"""[Colab]-Text-To-Image-BERTs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19EhRbPSPCCIwN1KvSV9ongYxgI4sM-Rd

## **Dataset**
"""

!gdown 1JJjMiNieTz7xYs6UeVqd02M3DW4fnEfU
!unzip cvpr2016_flowers.zip

import os

def load_captions(captions_folder, image_folder):
    captions = {}
    image_files = os.listdir(image_folder)
    for image_file in image_files:
        image_name = image_file.split('.')[0]
        caption_file = os.path.join(captions_folder, image_name + ".txt")
        with open(caption_file, "r") as f:
            caption = f.readlines()[0].strip()
        if image_name not in captions:
                captions[image_name] = caption
    return captions

captions_folder = "/content/content/cvpr2016_flowers/captions"
image_folder = "/content/content/cvpr2016_flowers/images"

captions = load_captions(captions_folder, image_folder)

"""## **Caption Encoder**"""

import torch
import numpy as np
from sentence_transformers import SentenceTransformer

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

bert_model = SentenceTransformer("all-mpnet-base-v2").to(device)

def encode_captions(captions):
    encoded_captions = {}
    for image_name in captions.keys():
        caption = captions[image_name]
        encoded_captions[image_name] = {
            'embed': torch.tensor(bert_model.encode(caption)),
            'text': caption
        }
    return encoded_captions

encoded_captions = encode_captions(captions)

"""## **Preprocessing**"""

from PIL import Image
from torch.utils.data import Dataset

class FlowerDataset(Dataset):
    def __init__(self, img_dir, captions, transform=None):
        self.img_dir = img_dir
        self.transform = transform

        # Load captions
        self.captions = captions

        self.img_names = list(self.captions.keys())

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.img_dir, img_name+".jpg")
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        encoded_caption = self.captions[img_name]['embed']
        caption = self.captions[img_name]['text']

        return {
            'image': image,
            'embed_caption': encoded_caption,
            'text': caption
        }

import torchvision.transforms as transforms

IMG_SIZE = 128

transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

ds = FlowerDataset(
    img_dir="/content/content/cvpr2016_flowers/images",
    captions=encoded_captions,
    transform=transform
)

import matplotlib.pyplot as plt

def show_grid(img):
  npimg = img.numpy()
  plt.imshow(np.transpose(npimg, (1, 2, 0)))
  plt.show()

show_grid(next(iter(ds))['wrong_image'])

from torch.utils.data import DataLoader

BATCH_SIZE = 1024
dataloader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)

batch_sample = next(iter(dataloader))

# import torchvision

# show_grid(torchvision.utils.make_grid(batch_sample['image'], normalize=True))

"""## **Model**"""

import torch.nn as nn

class Generator(nn.Module):
    def __init__(self, noise_size, feature_size, num_channels, embedding_size, reduced_dim_size):
        super(Generator, self).__init__()

        # Reduced dimension size (to encode the text embeddings into a smaller dimension)
        self.reduced_dim_size = reduced_dim_size

        # Text Encoder to reduce text embedding size
        self.textEncoder = nn.Sequential(
            nn.Linear(embedding_size, reduced_dim_size),          # Reduce the embedding to a smaller size
            nn.BatchNorm1d(reduced_dim_size),                    # Batch normalization for stability
            nn.LeakyReLU(negative_slope=0.2, inplace=True)       # LeakyReLU activation
        )

        # Upsampling Block to convert the noise and text embedding into an image
        self.upsamplingBlock = nn.Sequential(
            nn.ConvTranspose2d(noise_size + reduced_dim_size, feature_size * 8, 4, 1, 0, bias=False),  # 1st layer
            nn.BatchNorm2d(feature_size * 8),                # Batch normalization
            nn.LeakyReLU(negative_slope=0.2, inplace=True),  # LeakyReLU activation

            nn.ConvTranspose2d(feature_size * 8, feature_size * 4, 4, 2, 1, bias=False),  # 2nd layer
            nn.BatchNorm2d(feature_size * 4),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_size * 4, feature_size * 2, 4, 2, 1, bias=False),  # 3rd layer
            nn.BatchNorm2d(feature_size * 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_size * 2, feature_size, 4, 2, 1, bias=False),  # 4th layer
            nn.BatchNorm2d(feature_size),
            nn.ReLU(True),

            nn.ConvTranspose2d(feature_size, num_channels, 4, 2, 1, bias=False),  # Final layer to output the image
            nn.Tanh()  # Output image should have values between [-1, 1]
        )

    def forward(self, noise, text_embeddings):
        # Process the text embeddings through the text encoder
        encoded_text = self.textEncoder(text_embeddings)

        # Concatenate the noise and the encoded text embedding
        concat_input = torch.cat([noise, encoded_text], dim=1).unsqueeze(2).unsqueeze(2)

        # Pass through the upsampling block to generate the image
        output = self.upsamplingBlock(concat_input)

        return output

generator = Generator(100, 128, 3, 768, 256).to(device)

import torch.nn as nn

class Discriminator(nn.Module):
    def __init__(self, num_channels, feature_size, embedding_size, reduced_dim_size):
        super(Discriminator, self).__init__()

        # Reduced dimension size for encoding the text embedding
        self.reduced_dim_size = reduced_dim_size

        # Image Encoder to process image through convolutional layers
        self.imageEncoder = nn.Sequential(
            # Conv layer: 3 channels (RGB) -> feature_size channels
            nn.Conv2d(num_channels, feature_size, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU activation

            # Conv layer: feature_size channels -> feature_size channels
            nn.Conv2d(feature_size, feature_size, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # Conv layer: feature_size -> feature_size * 2 channels
            nn.Conv2d(feature_size, feature_size * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_size * 2),  # Batch normalization
            nn.LeakyReLU(0.2, inplace=True),

            # Conv layer: feature_size * 2 -> feature_size * 4 channels
            nn.Conv2d(feature_size * 2, feature_size * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_size * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # Conv layer: feature_size * 4 -> feature_size * 8 channels
            nn.Conv2d(feature_size * 4, feature_size * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_size * 8),
            nn.LeakyReLU(0.2, inplace=True),
        )

        # Text Encoder to reduce text embedding size and combine with image features
        self.textEncoder = nn.Sequential(
            nn.Linear(embedding_size, reduced_dim_size),  # Reduce embedding size
            nn.BatchNorm1d(reduced_dim_size),  # Batch normalization
            nn.LeakyReLU(negative_slope=0.2, inplace=True)  # LeakyReLU activation
        )

        # Final fully connected layers to output the result (real or fake)
        self.finalBlock = nn.Sequential(
            # Concatenate image features and text embedding
            nn.Conv2d(feature_size * 8 + reduced_dim_size, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()  # Sigmoid activation for the final classification (Real/Fake)
        )

    def forward(self, input_img, text_embeddings):
        # Process the image through the image encoder
        image_encoded = self.imageEncoder(input_img)

        # Process the text embedding through the text encoder
        text_encoded = self.textEncoder(text_embeddings)

        # Replicate the text embeddings to match image dimensions
        replicated_text = text_encoded.repeat(4, 4, 1, 1).permute(2, 3, 0, 1)

        # Concatenate the encoded image and text
        concat_layer = torch.cat([image_encoded, replicated_text], 1)

        # Final layer to produce output (real or fake)
        x = self.finalBlock(concat_layer)

        # Return the output and image features for later use
        return x.view(-1, 1), image_encoded

discriminator = Discriminator(3, 128, 768, 256).to(device)

"""## **Training**"""

bce_loss = nn.BCELoss()
l2_loss = nn.MSELoss()
l1_loss = nn.L1Loss()

plt_o_text_embeddings = ds[0]['embed_caption'].unsqueeze(0)
plt_o_text_embeddings.shape

fixed_noise = torch.randn(size=(1, 100))
fixed_noise.shape

import torchvision

show_grid(torchvision.utils.make_grid(ds[0]['image'], normalize=True))

def plot_output(generator):
  plt.clf()
  with torch.no_grad():

    generator.eval()
    test_images = generator(fixed_noise.to(device), plt_o_text_embeddings.to(device))
    generator.train()

    grid = torchvision.utils.make_grid(test_images.cpu(), normalize=True)
    show_grid(grid)

import torch.optim as optim

optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

import time

epochs = 500

for epoch in range(epochs):
    d_losses, g_losses = [], []  # Store the losses for discriminator and generator
    epoch_time = time.time()

    for batch in dataloader:
        # Get images and captions from the batch
        images = batch["image"].to(device)
        embed_captions = batch["embed_caption"].to(device)
        wrong_images = batch["wrong_image"].to(device)

        # Labels for real and fake images
        real_labels = torch.ones(images.size(0), 1, device=device)
        fake_labels = torch.zeros(images.size(0), 1, device=device)

        # -------------------
        # Train the Discriminator
        # -------------------

        # Zero the gradients of the Discriminator
        optimizer_D.zero_grad()

        # Generate fake images using the Generator
        noise = torch.randn(images.size(0), 100, device=device)
        fake_images = generator(noise, embed_captions)

        # Compute the loss for real images
        outputs, _ = discriminator(images, embed_captions)
        real_loss = bce_loss(outputs, real_labels)

        # Compute the loss for wrong images (for contrastive loss)
        outputs, _ = discriminator(wrong_images, embed_captions)
        wrong_loss = bce_loss(outputs, fake_labels)

        # Compute the loss for fake images
        outputs, _ = discriminator(fake_images.detach(), embed_captions)
        fake_loss = bce_loss(outputs, fake_labels)

        # Total loss for the Discriminator
        d_loss = real_loss + fake_loss + wrong_loss

        # Backpropagate the loss and update the Discriminator's weights
        d_loss.backward()
        optimizer_D.step()

        # Append the loss to track it
        d_losses.append(d_loss.item())

        # -------------------
        # Train the Generator
        # -------------------

        # Zero the gradients of the Generator
        optimizer_G.zero_grad()

        # Generate fake images again (using noise + captions)
        fake_images = generator(noise, embed_captions)

        # Get the Discriminator's outputs for the fake images
        outputs, fake_features = discriminator(fake_images, embed_captions)

        # Get the Discriminator's features for the real images
        _, real_features = discriminator(images, embed_captions)

        # Compute the loss for the Generator based on Discriminator's outputs
        activation_fake = torch.mean(fake_features, 0)
        activation_real = torch.mean(real_features, 0)

        real_loss = bce_loss(outputs, real_labels)
        g_loss = real_loss + 100 * l2_loss(activation_fake, activation_real.detach()) + 50 * l1_loss(fake_images, images)

        # Backpropagate the loss and update the Generator's weights
        g_loss.backward()
        optimizer_G.step()

        # Append the loss to track it
        g_losses.append(real_loss.item())

    # Calculate the average losses
    avg_d_loss = sum(d_losses) / len(d_losses)
    avg_g_loss = sum(g_losses) / len(g_losses)

    # Every 10 epochs, generate sample images to visualize progress
    if (epoch + 1) % 10 == 0:
        plot_output(generator)

    # Print the progress
    print('Epoch [{}/{}] loss_D: {:.4f} loss_G: {:.4f} time: {:.2f}'.format(
        epoch + 1, epochs,
        avg_d_loss,
        avg_g_loss,
        time.time() - epoch_time)
    )

"""## **Prediction**"""

generator.eval()

noise = torch.randn(size=(1, 100))
text_embedding = ds[10]['embed_caption'].unsqueeze(0)

with torch.no_grad():
    test_images = generator(noise.to(device), text_embedding.to(device))
grid = torchvision.utils.make_grid(test_images.cpu(), normalize=True)
show_grid(grid)

import torch

# Save generator
torch.save(generator.state_dict(), 'generator.pth')

# Save discriminator
torch.save(discriminator.state_dict(), 'discriminator.pth')