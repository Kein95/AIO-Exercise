# -*- coding: utf-8 -*-
"""[Colab]-Text-To-Image-DCGAN-Base-Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvUJtzgbxTZzvFJA-fuBralUh8UmC6I9
"""

import os
import urllib.request
import zipfile
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from nltk.tokenize import word_tokenize
from collections import Counter

# ===========================
# 0Ô∏è‚É£ T·∫£i v√† gi·∫£i n√©n Flickr8k
# ===========================
def download_flickr8k(dataset_dir="Flickr8k"):
    os.makedirs(dataset_dir, exist_ok=True)

    # Danh s√°ch c√°c URL c·∫ßn t·∫£i
    urls = {
        "images": "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip",
        "captions": "https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
    }

    for key, url in urls.items():
        zip_path = os.path.join(dataset_dir, f"{key}.zip")
        extract_path = os.path.join(dataset_dir, key)

        if not os.path.exists(extract_path):
            print(f"üì• Downloading {key} dataset...")
            urllib.request.urlretrieve(url, zip_path)

            print(f"üìÇ Extracting {key} dataset...")
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(dataset_dir)

            os.remove(zip_path)  # X√≥a file ZIP sau khi gi·∫£i n√©n

    print("‚úÖ Dataset downloaded & extracted!")

download_flickr8k()

import os

captions_file = "Flickr8k/Flickr8k.token.txt"
image_dir = "Flickr8k/Flicker8k_Dataset"  # Th∆∞ m·ª•c ch·ª©a ·∫£nh
captions = {}
text = []

with open(captions_file, "r") as f:
    for line in f:
        parts = line.strip().split("\t")
        img_name = parts[0].split("#")[0]

        # N·∫øu c√≥ ".1" ·ªü cu·ªëi file th√¨ lo·∫°i b·ªè
        if img_name.endswith(".1"):
            img_name = img_name[:-2]  # B·ªè k√Ω t·ª± ".1" ·ªü cu·ªëi

        # Ki·ªÉm tra xem file c√≥ t·ªìn t·∫°i kh√¥ng
        img_path = os.path.join(image_dir, img_name)
        if not os.path.exists(img_path):
            print(f"‚ö†Ô∏è File kh√¥ng t·ªìn t·∫°i: {img_name}")  # C·∫£nh b√°o file b·ªã thi·∫øu
            continue  # B·ªè qua file kh√¥ng t·ªìn t·∫°i

        caption = parts[1].lower()
        text.append(caption)

        if img_name not in captions:
            captions[img_name] = []
        captions[img_name].append(caption)

print("S·ªë l∆∞·ª£ng ·∫£nh h·ª£p l·ªá:", len(captions))

print(len(text))

captions['209605542_ca9cc52e7b.jpg']

text[0]

import os
from tokenizers import Tokenizer, pre_tokenizers, trainers, models

# T·∫°o tokenizer d·∫°ng word-based
tokenizer = Tokenizer(models.WordLevel(unk_token="<unk>"))

tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

trainer = trainers.WordLevelTrainer(
    vocab_size=10000,
    min_frequency=2,
    special_tokens=["<pad>", "<unk>"]
)

# Hu·∫•n luy·ªán tokenizer
tokenizer.train_from_iterator(text, trainer)

# L∆∞u tokenizer
tokenizer.save("tokenizer.json")

# Load t·ª´ ƒëi·ªÉn t·ª´ tokenizer
vocab = tokenizer.get_vocab()  # Tr√≠ch xu·∫•t t·ª´ ƒëi·ªÉn
word_to_id = lambda word: vocab.get(word, vocab["<unk>"])  # H√†m l·∫•y ID c·ªßa t·ª´

from transformers import PreTrainedTokenizerFast

# Load tokenizer ƒë√£ train v√†o PreTrainedTokenizerFast
tokenizer = PreTrainedTokenizerFast(
    tokenizer_file="tokenizer.json",
    unk_token="<unk>", pad_token="<pad>"
)

tokenizer("i go to school")

# ===========================
# 1Ô∏è‚É£ Load Flickr8k dataset
# ===========================
class Flickr8kDataset(Dataset):
    def __init__(self, img_dir, captions, transform=None):
        self.img_dir = img_dir
        self.transform = transform

        # Load captions
        self.captions = captions

        self.img_names = list(self.captions.keys())

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.img_dir, img_name)
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        caption = np.random.choice(self.captions[img_name])  # Ch·ªçn caption ng·∫´u nhi√™n
        encoded_caption = tokenizer(caption, padding="max_length",
                                    truncation=True, max_length=20, return_tensors="pt")['input_ids'][0]
        return {
            'image': image,
            'caption': encoded_caption
        }

transform = transforms.Compose([
    transforms.Resize((8, 8)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

dataset = Flickr8kDataset(
    img_dir="/content/Flickr8k/Flicker8k_Dataset",
    captions=captions,
    transform=transform
)

sample = next(iter(dataset))

sample

sample['image'].shape

sample['caption'].shape

# ===========================
# 2Ô∏è‚É£ Word Embeddings + LSTM
# ===========================
class embedding_text(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(embedding_text, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)

    def forward(self, captions):
        embeds = self.embedding(captions)
        _, (hidden, _) = self.lstm(embeds)
        return hidden[-1]

# ===========================
# 3Ô∏è‚É£ DCGAN Model
# ===========================

class Generator(nn.Module):
    def __init__(self, latent_dim, embed_dim):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim + embed_dim, 128 * 4 * 4),
            #       latent_dim (64) + embed_dim (256)
            # Input:(batch_size,320) - Output:(batch_size,512)
            nn.ReLU(True),

            nn.Unflatten(1, (128, 4, 4)),
            # Input:(batch_size,512) - Output:(batch_size,128,4,4)

            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            # Input:(batch_size,128,4,4) - Output:(batch_size,64, 8, 8)
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 3, 3, stride=1, padding=1),
            # Input:(batch_size,64, 8, 8) - Output:(batch_size, 3, 8, 8)
            nn.Tanh()
        )

    def forward(self, noise, caption_embed):
        x = torch.cat((noise, caption_embed), dim=1)
        #  (batch_size,64) + (batch_size,256) = (batch_size,320)
        return self.model(x)




class Discriminator(nn.Module):
    def __init__(self, embed_dim):
        super(Discriminator, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3, stride=2, padding=1),
            # Input:(batch_size,3,8,8) - Output:(batch_size,64,4,4)
            nn.LeakyReLU(0.2, inplace=True),

            nn.Conv2d(64 ,128 ,3 , stride=2, padding=1),
            # Input:(batch_size,64,4,4) - Output:(batch_size,128,2,2)
            nn.LeakyReLU(0.2, inplace=True),

            nn.Flatten()
            # Input:(batch_size,128,2,2) - Output:(batch_size,512)
        )

        self.fc = nn.Linear(512 + embed_dim, 1)
        # Input:(batch_size,512) - Output:(batch_size,1)

    def forward(self, img, caption_embed):
        img_features = self.cnn(img)
        x = torch.cat((img_features, caption_embed), dim=1)
        #  (batch_size,512) + (batch_size,256) = (batch_size,768)
        return torch.sigmoid(self.fc(x))

import nltk

nltk.download('punkt_tab')

len(tokenizer)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Model
generator = Generator(64, 256).to(device)
discriminator = Discriminator(256).to(device)
embedding_text = embedding_text(len(tokenizer), 256, 256).to(device)

optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
criterion = nn.BCELoss()

embedding_text

generator

discriminator

dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

batch_sample = next(iter(dataloader))

batch_sample

batch_sample['image'].shape

batch_sample['caption'].shape

caption_embeddings = embedding_text(batch_sample['caption'].to(device))

caption_embeddings.shape

noise = torch.randn(batch_sample['image'].size(0), 64, device=device)
fake_images = generator(noise, caption_embeddings)

fake_images.shape

for epoch in range(5):
    for batch in dataloader:
        # load tensor images
        images = batch['image'].to(device)
        # load one-hot vector tokenizer
        captions = batch['caption'].to(device)

        caption_embeddings = embedding_text(captions)
        #  (batch_size, 256)
        noise = torch.randn(images.size(0), 64, device=device)
        #  (batch_size, 64)

        fake_images = generator(noise, caption_embeddings)
        #  (batch_size, 3, 8, 8)
        real_labels = torch.ones(images.size(0), 1, device=device)
        #  (batch_size, 1)
        fake_labels = torch.zeros(images.size(0), 1, device=device)
        #  (batch_size, 1)

        real_loss = criterion(discriminator(images, caption_embeddings), real_labels)
        fake_loss = criterion(discriminator(fake_images.detach(), caption_embeddings), fake_labels)
        '''    Detaches fake_images from the computation graph so that gradients
        do not flow back to the generator during Discriminator training.     '''

        d_loss = real_loss + fake_loss
        optimizer_D.zero_grad()
        d_loss.backward(retain_graph=True)
        #  retain_graph=True
        optimizer_D.step()

        g_loss = criterion(discriminator(fake_images, caption_embeddings), real_labels)

        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()

    print(f"Epoch [{epoch+1}/5], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}")