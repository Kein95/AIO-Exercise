# -*- coding: utf-8 -*-
"""M3_W3_Decision_Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qJpdyIQoMj0hD0qaqyXlAaX1XaldLjyr

#1. Decision Tree for Classification
"""

import numpy as np
from collections import Counter

# Bộ dữ liệu từ đề bài
dataset = [
    [23, 0, 0, 0],
    [25, 1, 1, 0],
    [27, 1, 0, 1],
    [29, 0, 1, 1],
    [29, 0, 0, 0]
]

# Các lớp phân loại
class_values = list(set(row[-1] for row in dataset))

# Hàm chia tập dữ liệu theo giá trị thuộc tính
def split_dataset(dataset, index, value):
    left, right = list(), list()
    for row in dataset:
        if row[index] <= value:
            left.append(row)
        else:
            right.append(row)
    return left, right

# Tính Entropy cho toàn bộ tập dữ liệu D (trước khi chia)
def entropy_of_dataset(dataset):
    label_column = [row[-1] for row in dataset]
    total_samples = len(label_column)
    class_counts = Counter(label_column)
    entropy = 0.0
    for class_val in class_counts:
        p = class_counts[class_val] / total_samples
        if p > 0:
            entropy -= p * np.log2(p)
    return round(entropy, 2)

# Hàm tính Gini
def gini_index(groups, classes):
    n_instances = float(sum([len(group) for group in groups]))
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        class_counts = Counter([row[-1] for row in group])
        for class_val in classes:
            p = class_counts[class_val] / size
            score += p * p
        gini += (1.0 - score) * (size / n_instances)
    return round(gini, 2)

# Hàm tính Entropy
def entropy(groups, classes):
    n_instances = float(sum([len(group) for group in groups]))
    entropy = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        class_counts = Counter([row[-1] for row in group])
        for class_val in classes:
            p = class_counts[class_val] / size
            if p > 0:
                score -= p * np.log2(p)
        entropy += score * (size / n_instances)
    return round(entropy, 2)

# Q2 Tính Gini cho cột nhãn "Raise Salary"
gini_raise_salary = gini_for_label_column(dataset)
print(f'Gini của các mẫu trong cột nhãn "Raise Salary": {gini_raise_salary:.2f}')

# Q4 Tính Gini khi thuộc tính 'Age <= 26'
groups = split_dataset(dataset, 0, 26)
gini_age = gini_index(groups, class_values)
print(f'Gini của thuộc tính "Age <= 26": {gini_age:.2f}')

# Q5 Tính Entropy cho toàn bộ cột nhãn 'Raise Salary'
groups = [dataset]
entropy_value = entropy(groups, class_values)
print(f'Entropy của toàn bộ cột "Raise Salary": {entropy_value:.2f}')

# Q6 Tính Gain cho thuộc tính 'Likes English'
gain_likes_english = calculate_gain(dataset, 1)
print(f'Gain của thuộc tính "Likes English": {gain_likes_english:.3f}')

# Q7
from sklearn import datasets

# Tải về bộ dữ liệu Iris
iris_X, iris_y = datasets.load_iris(return_X_y=True)

# Q8
from sklearn import datasets
from sklearn . model_selection import train_test_split
from sklearn . metrics import accuracy_score
from sklearn . tree import DecisionTreeClassifier

# Paragraph C :
# Load the diabetes dataset
iris_X, iris_y = datasets.load_iris(return_X_y=True) # From question 7
# Split train : test = 8:2
X_train, X_test, y_train, y_test = train_test_split(
    iris_X, iris_y,
    test_size=0.2,
    random_state=42)

# Paragraph B :
# Define model
dt_classifier = DecisionTreeClassifier()

# Paragraph A :
# Train
dt_classifier.fit(X_train, y_train)

# Paragraph D :
# Preidct and evaluate
y_pred = dt_classifier.predict(X_test)
accuracy_score(y_test, y_pred)

"""#2. Decision Tree for Regression"""

import numpy as np

# Dữ liệu từ bảng
data = [
    {'Age': 23, 'Likes English': 0, 'Likes AI': 0, 'Salary': 200},
    {'Age': 25, 'Likes English': 1, 'Likes AI': 1, 'Salary': 400},
    {'Age': 27, 'Likes English': 1, 'Likes AI': 0, 'Salary': 300},
    {'Age': 29, 'Likes English': 0, 'Likes AI': 1, 'Salary': 500},
    {'Age': 29, 'Likes English': 0, 'Likes AI': 0, 'Salary': 400}
]

# Hàm tính SSE
def calculate_sse(data, condition):
    subset = [d['Salary'] for d in data if condition(d)]
    mean_value = np.mean(subset)
    sse = np.sum((np.array(subset) - mean_value) ** 2)
    return sse

# SSE cho 'Likes AI'
sse_likes_ai_true = calculate_sse(data, lambda d: d['Likes AI'] == 1)
sse_likes_ai_false = calculate_sse(data, lambda d: d['Likes AI'] == 0)
sse_likes_ai = sse_likes_ai_true + sse_likes_ai_false

print(f"SSE(Likes AI) = {sse_likes_ai}")

# SSE cho 'Age <= 24'
sse_age_leq_24 = calculate_sse(data, lambda d: d['Age'] <= 24)
sse_age_gt_24 = calculate_sse(data, lambda d: d['Age'] > 24)
sse_age = sse_age_leq_24 + sse_age_gt_24

print(f"SSE(Age <= 24) = {sse_age}")

from sklearn . datasets import fetch_openml
from sklearn . model_selection import train_test_split
from sklearn . metrics import mean_squared_error
from sklearn . tree import DecisionTreeRegressor

# Paragraph C :
# Load dataset
machine_cpu = fetch_openml ( name ='machine_cpu')
machine_data = machine_cpu . data
machine_labels = machine_cpu . target
# Split train : test = 8:2
X_train , X_test , y_train , y_test = train_test_split (
machine_data , machine_labels ,
test_size =0.2 ,
random_state =42)

# Paragraph B :
# Define model
tree_reg = DecisionTreeRegressor ()

# Paragraph A :
# Train
tree_reg . fit ( X_train , y_train )

# Paragraph D :
# Preidct and evaluate
y_pred = tree_reg . predict ( X_test )
mean_squared_error ( y_test , y_pred )