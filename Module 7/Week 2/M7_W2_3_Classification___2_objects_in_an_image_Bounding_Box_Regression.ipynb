{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG2DTFXKJndV",
        "outputId": "1c9f3a2c-1b45-4825-edec-186ed603e4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(\"Path to dataset files:\", data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.patches as patches\n",
        "import xml.etree.ElementTree as ET\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n"
      ],
      "metadata": {
        "id": "TNhH2QpVKRMU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Get image size for normalization\n",
        "        image_width = int(root.find(\"size/width\").text)\n",
        "        image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:  # Take the first label\n",
        "                label = name\n",
        "            # Get bounding box coordinates\n",
        "            xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "            ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "            xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "            ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "            # Normalize bbox coordinates to [0, 1]\n",
        "            bbox = [\n",
        "                xmin / image_width,\n",
        "                ymin / image_height,\n",
        "                xmax / image_width,\n",
        "                ymax / image_height,\n",
        "            ]\n",
        "\n",
        "        # Convert label to numerical representation (0 for cat, 1 for dog)\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "\n",
        "        return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1_file = self.image_files[idx]\n",
        "        img1_path = os.path.join(self.image_dir, img1_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img1_file)[0] + \".xml\"\n",
        "        img1_annotations = self.parse_annotation(\n",
        "            os.path.join(self.annotations_dir, annotation_name)\n",
        "        )\n",
        "\n",
        "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
        "        img2_file = self.image_files[idx2]\n",
        "        img2_path = os.path.join(self.image_dir, img2_file)\n",
        "\n",
        "        annotation_name = os.path.splitext(img2_file)[0] + \".xml\"\n",
        "        img2_annotations = self.parse_annotation(\n",
        "            os.path.join(self.annotations_dir, annotation_name)\n",
        "        )\n",
        "\n",
        "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
        "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
        "\n",
        "        # Horizontal merge\n",
        "        merged_image = Image.new(\n",
        "            \"RGB\", (img1.width + img2.width, max(img1.height, img2.height))\n",
        "        )\n",
        "        merged_image.paste(img1, (0, 0))\n",
        "        merged_image.paste(img2, (img1.width, 0))\n",
        "        merged_w = img1.width + img2.width\n",
        "        merged_h = max(img1.height, img2.height)\n",
        "\n",
        "        merged_annotations = []\n",
        "\n",
        "        # No change for objects from img1, already normalized\n",
        "        merged_annotations.append(\n",
        "            {\"bbox\": img1_annotations[1].tolist(), \"label\": img1_annotations[0]}\n",
        "        )\n",
        "\n",
        "        # Adjust bbox coordinates for objects from img2 AND normalize\n",
        "        new_bbox = [\n",
        "            (img2_annotations[1][0] * img2.width + img1.width) / merged_w,  # Normalize xmin\n",
        "            img2_annotations[1][1] * img2.height / merged_h,  # Normalize ymin\n",
        "            (img2_annotations[1][2] * img2.width + img1.width) / merged_w,  # Normalize xmax\n",
        "            img2_annotations[1][3] * img2.height / merged_h,  # Normalize ymax\n",
        "        ]\n",
        "\n",
        "        merged_annotations.append({\"bbox\": new_bbox, \"label\": img2_annotations[0]})\n",
        "\n",
        "        # Convert merged image to tensor\n",
        "        if self.transform:\n",
        "            merged_image = self.transform(merged_image)\n",
        "        else:\n",
        "            merged_image = transforms.ToTensor()(merged_image)\n",
        "\n",
        "        # Convert annotations to 1D tensors, with shape (4,) for bbox and (1,) for label\n",
        "        annotations = torch.zeros((len(merged_annotations), 5))\n",
        "        for i, ann in enumerate(merged_annotations):\n",
        "            annotations[i] = torch.cat(\n",
        "                (torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]]))\n",
        "            )\n",
        "\n",
        "        return merged_image, annotations\n"
      ],
      "metadata": {
        "id": "NOhWAIfBKgJX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data directory\n",
        "annotations_dir = os.path.join(data_dir, 'annotations')\n",
        "image_dir = os.path.join(data_dir, 'images')\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "dataset = MyDataset(annotations_dir, image_dir, transform=transform)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "kdX7FM7zK04q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleYOLO(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleYOLO, self).__init__()\n",
        "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Remove the final classification layer of ResNet\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Add the YOLO head\n",
        "        self.fcs = nn.Linear(\n",
        "            2048, 2 * 2 * (4 + self.num_classes)\n",
        "        )  # 2 is for the number of grid cell\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, C, H, W)\n",
        "        features = self.backbone(x)\n",
        "        features = F.adaptive_avg_pool2d(features, (1, 1))  # shape: (batch_size, 2048, 1, 1)\n",
        "        features = features.view(features.size(0), -1)  # shape: (batch_size, 2048)\n",
        "        features = self.fcs(features)\n",
        "\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "nDidRheDLmzq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_classes = 2  # Assuming two classes: dog and cat\n",
        "class_to_idx = {'dog': 0, 'cat': 1}\n",
        "\n",
        "model = SimpleYOLO(num_classes=num_classes).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "Qq8uYA4wOpjz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(output, targets, device, num_classes):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_size = output.shape[0]\n",
        "    total_loss = 0\n",
        "\n",
        "    output = output.view(batch_size, 2, 2, 4 + num_classes)  # Reshape to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
        "\n",
        "    for i in range(batch_size):  # Iterate through each image in the batch\n",
        "        for j in range(len(targets[i])):  # Iterate through objects in the image\n",
        "\n",
        "            # Determine which grid cell the object's center falls into\n",
        "            # Assuming bbox coordinates are normalized to [0, 1]\n",
        "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
        "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
        "\n",
        "            grid_x = int(bbox_center_x * 2)  # Multiply by number of grid cells (2 in this case)\n",
        "            grid_y = int(bbox_center_y * 2)\n",
        "\n",
        "            # 1. Classification Loss for the responsible grid cell\n",
        "            # Convert label to one-hot encoding only for this example\n",
        "            # One hot encoding\n",
        "            label_one_hot = torch.zeros(num_classes, device=device)\n",
        "            label_one_hot[int(targets[i][j][4])] = 1\n",
        "\n",
        "            # Classification loss (using CrossEntropyLoss)\n",
        "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
        "\n",
        "            # 2. Regression Loss for the responsible grid cell\n",
        "            bbox_target = targets[i][j][:4].to(device)\n",
        "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
        "\n",
        "            # 3. No Object Loss (for other grid cells)\n",
        "            no_obj_loss = 0\n",
        "            for other_grid_y in range(2):\n",
        "                for other_grid_x in range(2):\n",
        "                    if other_grid_y != grid_y or other_grid_x != grid_x:\n",
        "                        # MSE loss for predicting no object (all zeros)\n",
        "                        no_obj_loss += mse_loss(output[i, other_grid_y, other_grid_x, :4], torch.zeros(4, device=device))\n",
        "\n",
        "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
        "\n",
        "    return total_loss / batch_size  # Average loss over the batch\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm.tqdm(data_loader, desc=\"Validation\", leave=False):\n",
        "            images = images.to(device)\n",
        "\n",
        "            output = model(images)\n",
        "\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "            # Reshape output to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
        "            output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
        "\n",
        "            # Collect predictions and targets for mAP calculation\n",
        "            for batch_idx in range(images.shape[0]):\n",
        "                for target in targets[batch_idx]:\n",
        "                    # Determine responsible grid cell\n",
        "                    bbox_center_x = (target[0] + target[2]) / 2\n",
        "                    bbox_center_y = (target[1] + target[3]) / 2\n",
        "                    grid_x = int(bbox_center_x * 2)\n",
        "                    grid_y = int(bbox_center_y * 2)\n",
        "\n",
        "                    # Class prediction (index of max probability)\n",
        "                    prediction = output[batch_idx, grid_y, grid_x, 4:].argmax().item()\n",
        "                    all_predictions.append(prediction)\n",
        "\n",
        "                    all_targets.append(target[4].item())\n",
        "\n",
        "    val_loss = running_loss / len(data_loader)\n",
        "\n",
        "    # Convert lists to tensors for PyTorch's metric functions\n",
        "    all_predictions = torch.tensor(all_predictions, device=device)\n",
        "    all_targets = torch.tensor(all_targets, device=device)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = (all_predictions == all_targets).float().mean()\n",
        "\n",
        "    return val_loss, val_accuracy.item()\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes):\n",
        "    best_val_accuracy = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, targets in tqdm.tqdm(train_loader, desc=\"Batches\", leave=False):\n",
        "            images = images.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(images)\n",
        "\n",
        "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "            f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Save the best model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "9SiqMnU6LqqN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu trọng số mô hình\n",
        "torch.save(model.state_dict(), \"best_model.pth\")\n",
        "print(\"✅ Mô hình đã được lưu thành công!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2pt3Aj8PB1T",
        "outputId": "44112e4a-c58d-4bc5-d47a-62e108108a18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Mô hình đã được lưu thành công!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, image_path, transform, device, class_to_idx, threshold=0.5):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_width, original_height = image.size\n",
        "\n",
        "    # Resize the image to match the input size expected by the model (e.g., 448x448)\n",
        "    resized_image = image.resize((448, 448))\n",
        "    resized_width, resized_height = resized_image.size\n",
        "\n",
        "    # Apply the same transformations used during training\n",
        "    transformed_image = transform(resized_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed_image)\n",
        "        output = output.view(1, 2, 2, 4 + len(class_to_idx))  # Reshape for 2x2 grid\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.axis(\"off\")\n",
        "    ax.imshow(resized_image)  # Display resized image\n",
        "\n",
        "    for grid_y in range(2):\n",
        "        for grid_x in range(2):\n",
        "            # Get the class prediction and bounding box for the current grid cell\n",
        "            class_pred = output[0, grid_y, grid_x, 4:].argmax().item()\n",
        "            bbox = output[0, grid_y, grid_x, :4].tolist()  # Predicted bbox\n",
        "\n",
        "            # Confidence (probability of the predicted class)\n",
        "            confidence = torch.softmax(output[0, grid_y, grid_x, 4:], dim=0)[\n",
        "                class_pred\n",
        "            ].item()\n",
        "\n",
        "            # Scale the bounding box back to the resized image size\n",
        "            # Assuming bbox coordinates are normalized to [0,1] within the grid cell\n",
        "            x_min = bbox[0] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_min = bbox[1] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "            x_max = bbox[2] * (resized_width / 2) + grid_x * (resized_width / 2)\n",
        "            y_max = bbox[3] * (resized_height / 2) + grid_y * (resized_height / 2)\n",
        "\n",
        "            # Draw the bounding box and label on the image if confidence is above threshold\n",
        "            if confidence > threshold:\n",
        "                rect = patches.Rectangle(\n",
        "                    (x_min, y_min),\n",
        "                    x_max - x_min,\n",
        "                    y_max - y_min,\n",
        "                    linewidth=1,\n",
        "                    edgecolor=\"r\",\n",
        "                    facecolor=\"none\",\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "                plt.text(\n",
        "                    x_min,\n",
        "                    y_min,\n",
        "                    f\"{list(class_to_idx.keys())[class_pred]}: {confidence:.2f}\",\n",
        "                    color=\"white\",\n",
        "                    fontsize=12,\n",
        "                    bbox=dict(facecolor=\"red\", alpha=0.5),\n",
        "                )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "\n",
        "# Inference on a sample image\n",
        "# image_path = os.path.join(image_dir, \"cat.100.jpg\")\n",
        "image_path = \"/mnt/c/Study/OD Project/good_1.jpg\"\n",
        "inference(model, image_path, transform, device, class_to_idx, threshold=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "Pgx0ohhKNtiw",
        "outputId": "fd157ec5-8688-4831-feb7-6db1d89011ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-e2860703376d>:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/c/Study/OD Project/good_1.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e2860703376d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# image_path = os.path.join(image_dir, \"cat.100.jpg\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/c/Study/OD Project/good_1.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-e2860703376d>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, image_path, transform, device, class_to_idx, threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moriginal_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/c/Study/OD Project/good_1.jpg'"
          ]
        }
      ]
    }
  ]
}