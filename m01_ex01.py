# -*- coding: utf-8 -*-
"""M01_EX01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y2MFr-RbPJGKDHA85QqzeUrE7x5rC7xz
"""

# Import libraries
import math
import random

"""1. Viết function thực hiện đánh giá classification model bằng F1-Score."""

def evaluate_classification_model(tp, fp, fn):
    # Kiểm tra các giá trị đầu vào
    if not all(isinstance(i, int) for i in [tp, fp, fn]):
        print("tp, fp, and fn must be int")
        return

    if tp <= 0 and fp <= 0 and fn <= 0:
        print("tp and fp and fn must be greater than zero")
        return

    # Tính toán Precision, Recall và F1-Score
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1_score = 2 * (precision * recall) / (precision + recall)

    # In kết quả
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1-Score:", f1_score)

evaluate_classification_model(2, 4, 5)

"""2. Viết function mô phỏng theo 3 activation function."""

def is_number ( n ) :
  try :
    float ( n )
    return True
  except ValueError :
    return False

def activation_function(x, activation_name):
  # Kiểm tra x có hợp lệ hay không
  if not is_number(x):
    print("x must be a number")
    return
  # Kiểm tra tên activation function
  match activation_name:
    case 'sigmoid':
      return 1 / (1 + math.exp(-x))
    case 'relu':
      return max(0, x)
    case 'elu':
      return x if x>0 else 0.01 * (math.exp(x)-1)
    case _:
      print(f"{activation_name} is not supported")
      return



activation_function('abc', 'sigmoid')

activation_function(1.5,'belu')

activation_function(2, 'relu')