# -*- coding: utf-8 -*-
"""M10-W1-P3-llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N5ztqmWY2e7WTk4vVkbCvDafS_9rxr2s
"""

!pip install -qq --upgrade pip
!pip install -qq --upgrade peft transformers accelerate bitsandbytes datasets trl

from datasets import Dataset
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("unsloth/Llama-3.2-1B-Instruct", cache_dir="./cache")

chat = [
    # {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello, how are you?"},
    {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
    {"role": "user", "content": "I'd like to show off how chat templating works!"},
    {"role": "assistant", "content": "Sure! What would you like me to say?"}
]

chat_prompt = tokenizer.apply_chat_template(chat, tokenize=False)
print(chat_prompt)

chats = [
    [
        {"role": "user", "content": "Hello, how are you?"},
        {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
        {"role": "user", "content": "I'd like to show off how chat templating works!"},
        {"role": "assistant", "content": "Sure! What would you like me to say?"}
    ],
    [
        {"role": "user", "content": "Which is bigger, the moon or the sun?"},
        {"role": "assistant", "content": "The sun."}
    ],
    [
        {"role": "user", "content": "Which is bigger, a virus or a bacterium?"},
        {"role": "assistant", "content": "A bacterium."}
    ],
]

dataset = Dataset.from_dict({"chats": chats})
dataset

formatted_dataset = dataset.map(lambda x: {"formatted_chat": tokenizer.apply_chat_template(x["chats"], tokenize=False, add_generation_prompt=False, add_special_tokens=False)})
print(formatted_dataset['formatted_chat'])

messages_field = "chats"  # Replace with the actual field name
def format_dataset(examples):
    if isinstance(examples[messages_field][0], list):
        output_texts = []
        for i in range(len(examples[messages_field])):
            output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))
        return {"formatted_text": output_texts}
    else:
        return {"formatted_text": tokenizer.apply_chat_template(examples[messages_field], tokenize=False)}

formatted_dataset = dataset.map(format_dataset)

tokenized_sample = tokenizer(formatted_dataset['formatted_text'][0], padding=True, truncation=True, return_special_tokens_mask=True, add_special_tokens=False)
tokenized_sample,

tokenizer.decode(tokenized_sample['input_ids'], skip_special_tokens=False)

"""## References

- https://huggingface.co/docs/transformers/main/en/chat_templating
- https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L65
- https://gist.github.com/Blaizzy/40de0f6b4340490e3920db9e182e6455
- https://www.kaggle.com/code/lonnieqin/instruction-tuning-with-llm
- https://openreview.net/pdf?id=GcZgo9ffGt
"""

from datasets import load_dataset

tmp_dataset = load_dataset("trl-lib/Capybara", split="train")
tmp_dataset, tmp_dataset[0]

