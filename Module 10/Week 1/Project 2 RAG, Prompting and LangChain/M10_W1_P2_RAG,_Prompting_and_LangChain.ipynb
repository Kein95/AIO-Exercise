{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "78WXPNWknf6P"
      },
      "outputs": [],
      "source": [
        "# Tạo cấu trúc thư mục chuẩn\n",
        "!mkdir -p rag_langchain/data_source/generative_ai\n",
        "!mkdir -p rag_langchain/src/base\n",
        "!mkdir -p rag_langchain/src/rag\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/data_source/generative_ai/download.py\n",
        "import os\n",
        "import wget\n",
        "\n",
        "file_links = [\n",
        "    {\"title\": \"Attention Is All You Need\", \"url\": \"https://arxiv.org/pdf/1706.03762\"},\n",
        "    {\"title\": \"BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"url\": \"https://arxiv.org/pdf/1810.04805\"},\n",
        "    {\"title\": \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", \"url\": \"https://arxiv.org/pdf/2201.11903\"},\n",
        "    {\"title\": \"Denoising Diffusion Probabilistic Models\", \"url\": \"https://arxiv.org/pdf/2006.11239\"},\n",
        "    {\"title\": \"Instruction Tuning for Large Language Models - A Survey\", \"url\": \"https://arxiv.org/pdf/2308.10792\"},\n",
        "    {\"title\": \"Llama 2- Open Foundation and Fine-Tuned Chat Models\", \"url\": \"https://arxiv.org/pdf/2307.09288\"}\n",
        "]\n",
        "\n",
        "def is_exist(file_link):\n",
        "    return os.path.exists(f\"./{file_link['title']}.pdf\")\n",
        "\n",
        "for file_link in file_links:\n",
        "    if not is_exist(file_link):\n",
        "        wget.download(file_link[\"url\"], out=f\"./{file_link['title']}.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6lt13rfntr_",
        "outputId": "677e97df-f332-4b26-8a79-d0191f0c68dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/data_source/generative_ai/download.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/base/llm_model.py\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "# Cấu hình quantization cho mô hình\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "def get_hf_llm(model_name: str = \"meta-llama/Llama-3-8B-Instruct\", max_new_token=1024):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=max_new_token,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(\n",
        "        pipeline=model_pipeline\n",
        "    )\n",
        "\n",
        "    return llm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3ZaQ3AWn35p",
        "outputId": "c3ba154f-2e88-4598-887f-010aae453647"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/base/llm_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/rag/file_loader.py\n",
        "from typing import Union, List, Literal\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Hàm để loại bỏ các ký tự không phải UTF-8\n",
        "def remove_non_utf8_characters(text):\n",
        "    return ''.join(char for char in text if ord(char) < 128)\n",
        "\n",
        "# Hàm load PDF\n",
        "def load_pdf(pdf_file):\n",
        "    docs = PyPDFLoader(pdf_file, extract_images=True).load()\n",
        "    for doc in docs:\n",
        "        doc.page_content = remove_non_utf8_characters(doc.page_content)\n",
        "    return docs\n",
        "\n",
        "# Hàm lấy số CPU\n",
        "def get_num_cpu():\n",
        "    return multiprocessing.cpu_count()\n",
        "\n",
        "# Base loader class\n",
        "class BaseLoader:\n",
        "    def __init__(self) -> None:\n",
        "        self.num_processes = get_num_cpu()\n",
        "\n",
        "    def __call__(self, files: List[str], **kwargs):\n",
        "        pass\n",
        "\n",
        "# PDF loader class\n",
        "class PDFLoader(BaseLoader):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, pdf_files: List[str], **kwargs):\n",
        "        num_processes = min(self.num_processes, kwargs[\"workers\"])\n",
        "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
        "            doc_loaded = []\n",
        "            total_files = len(pdf_files)\n",
        "            with tqdm(total=total_files, desc=\"Loading PDFs\", unit=\"file\") as pbar:\n",
        "                for result in pool.imap_unordered(load_pdf, pdf_files):\n",
        "                    doc_loaded.extend(result)\n",
        "                    pbar.update(1)\n",
        "        return doc_loaded\n",
        "\n",
        "# TextSplitter class\n",
        "class TextSplitter:\n",
        "    def __init__(self, separators: List[str] = ['\\n\\n', '\\n', ' ', ''], chunk_size: int = 300, chunk_overlap: int = 0) -> None:\n",
        "        self.splitter = RecursiveCharacterTextSplitter(\n",
        "            separators=separators,\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "        )\n",
        "\n",
        "    def __call__(self, documents):\n",
        "        return self.splitter.split_documents(documents)\n",
        "\n",
        "# Loader class\n",
        "class Loader:\n",
        "    def __init__(self, file_type: str = Literal[\"pdf\"], split_kwargs: dict = {\"chunk_size\": 300, \"chunk_overlap\": 0}) -> None:\n",
        "        assert file_type in [\"pdf\"], \"file_type must be pdf\"\n",
        "        self.file_type = file_type\n",
        "        if file_type == \"pdf\":\n",
        "            self.doc_loader = PDFLoader()\n",
        "        else:\n",
        "            raise ValueError(\"file_type must be pdf\")\n",
        "\n",
        "        self.doc_spltter = TextSplitter(**split_kwargs)\n",
        "\n",
        "    def load(self, pdf_files: Union[str, List[str]], workers: int = 1):\n",
        "        if isinstance(pdf_files, str):\n",
        "            pdf_files = [pdf_files]\n",
        "        doc_loaded = self.doc_loader(pdf_files, workers=workers)\n",
        "        doc_split = self.doc_spltter(doc_loaded)\n",
        "        return doc_split\n",
        "\n",
        "    def load_dir(self, dir_path: str, workers: int = 1):\n",
        "        if self.file_type == \"pdf\":\n",
        "            files = glob.glob(f\"{dir_path}/*.pdf\")\n",
        "            assert len(files) > 0, f\"No {self.file_type} files found in {dir_path}\"\n",
        "        else:\n",
        "            raise ValueError(\"file_type must be pdf\")\n",
        "\n",
        "        return self.load(files, workers=workers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQfE6kSXn5Yb",
        "outputId": "23a4a599-8a16-48cf-e33c-e6de0eb06605"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/rag/file_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/rag/vectorstore.py\n",
        "from typing import Union\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "class VectorDB:\n",
        "    def __init__(self,\n",
        "                 documents=None,\n",
        "                 vector_db: Union[Chroma, FAISS] = Chroma,\n",
        "                 embedding=HuggingFaceEmbeddings()) -> None:\n",
        "        self.vector_db = vector_db\n",
        "        self.embedding = embedding\n",
        "        self.db = self._build_db(documents)\n",
        "\n",
        "    def _build_db(self, documents):\n",
        "        db = self.vector_db.from_documents(documents=documents, embedding=self.embedding)\n",
        "        return db\n",
        "\n",
        "    def get_retriever(self, search_type: str = \"similarity\", search_kwargs: dict = {\"k\": 10}):\n",
        "        retriever = self.db.as_retriever(search_type=search_type, search_kwargs=search_kwargs)\n",
        "        return retriever\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqYMWcg_n-0Y",
        "outputId": "dbaa9cf3-4a8f-4bd2-b369-8474147ebfcc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/rag/vectorstore.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/rag/offline_rag.py\n",
        "import re\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class Str_OutputParser(StrOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def parse(self, text: str) -> str:\n",
        "        return self.extract_answer(text)\n",
        "\n",
        "    def extract_answer(self,\n",
        "                       text_response: str,\n",
        "                       pattern: str = r\"Answer :\\s*(.*)\") -> str:\n",
        "        match = re.search(pattern, text_response, re.DOTALL)\n",
        "        if match:\n",
        "            answer_text = match.group(1).strip()\n",
        "            return answer_text\n",
        "        else:\n",
        "            return text_response\n",
        "\n",
        "class Offline_RAG:\n",
        "    def __init__(self, llm) -> None:\n",
        "        self.llm = llm\n",
        "        self.prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "        self.str_parser = Str_OutputParser()\n",
        "\n",
        "    def get_chain(self, retriever):\n",
        "        input_data = {\n",
        "            \"context\": retriever | self.format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "        rag_chain = (\n",
        "            input_data\n",
        "            | self.prompt\n",
        "            | self.llm\n",
        "            | self.str_parser\n",
        "        )\n",
        "        return rag_chain\n",
        "\n",
        "    def format_docs(self, docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD8-OHe3oDPJ",
        "outputId": "fc937280-1f46-4a51-a03f-b6baa9043b82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/rag/offline_rag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/rag/utils.py\n",
        "import re\n",
        "\n",
        "def extract_answer(text_response: str, pattern: str = r\"Answer :\\s*(.*)\") -> str:\n",
        "    match = re.search(pattern, text_response)\n",
        "    if match:\n",
        "        answer_text = match.group(1).strip()\n",
        "        return answer_text\n",
        "    else:\n",
        "        return \"Answer not found.\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSLlBFmUoEB-",
        "outputId": "4fa78ab8-f60d-4342-cc77-9f9de6d5ff5d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/rag/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/rag/main.py\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from src.rag.file_loader import Loader\n",
        "from src.rag.vectorstore import VectorDB\n",
        "from src.rag.offline_rag import Offline_RAG\n",
        "\n",
        "# Lớp InputQA dùng để xác định câu hỏi người dùng\n",
        "class InputQA(BaseModel):\n",
        "    question: str = Field(..., title=\"Question to ask the model\")\n",
        "\n",
        "# Lớp OutputQA dùng để xác định câu trả lời từ mô hình\n",
        "class OutputQA(BaseModel):\n",
        "    answer: str = Field(..., title=\"Answer from the model\")\n",
        "\n",
        "# Hàm build_rag_chain khởi tạo toàn bộ quy trình RAG\n",
        "def build_rag_chain(llm, data_dir, data_type):\n",
        "    # Tải dữ liệu và chia nhỏ thành các tài liệu\n",
        "    doc_loaded = Loader(file_type=data_type).load_dir(data_dir, workers=2)\n",
        "\n",
        "    # Tạo cơ sở dữ liệu vector và retriever\n",
        "    retriever = VectorDB(documents=doc_loaded).get_retriever()\n",
        "\n",
        "    # Xây dựng RAG chain\n",
        "    rag_chain = Offline_RAG(llm).get_chain(retriever)\n",
        "\n",
        "    return rag_chain\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uB2LpL6oHZi",
        "outputId": "5e51a4b9-71ae-4b58-ab9b-6c83deab6abf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/rag/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/src/app.py\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "from langserve import add_routes\n",
        "from src.base.llm_model import get_hf_llm\n",
        "from src.rag.main import build_rag_chain, InputQA, OutputQA\n",
        "\n",
        "# Khởi tạo mô hình LLM\n",
        "llm = get_hf_llm(temperature=0.9)\n",
        "\n",
        "# Đường dẫn tới tài liệu\n",
        "genai_docs = \"./data_source/generative_ai\"\n",
        "\n",
        "# --------- Chains -------------------\n",
        "genai_chain = build_rag_chain(llm, data_dir=genai_docs, data_type=\"pdf\")\n",
        "\n",
        "# --------- App - FastAPI ----------------\n",
        "app = FastAPI(\n",
        "    title=\"LangChain Server\",\n",
        "    version=\"1.0\",\n",
        "    description=\"A simple api server using Langchain’s Runnable interfaces\",\n",
        ")\n",
        "\n",
        "# Cấu hình CORS cho phép truy cập từ tất cả các nguồn\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        "    expose_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# --------- Routes - FastAPI ----------------\n",
        "\n",
        "# Endpoint kiểm tra trạng thái server\n",
        "@app.get(\"/check\")\n",
        "async def check():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# Endpoint xử lý câu hỏi và trả về câu trả lời\n",
        "@app.post(\"/generative_ai\", response_model=OutputQA)\n",
        "async def generative_ai(inputs: InputQA):\n",
        "    answer = genai_chain.invoke(inputs.question)\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "# --------- Langserve Routes - Playground ----------------\n",
        "add_routes(\n",
        "    app,\n",
        "    genai_chain,\n",
        "    playground_type=\"default\",\n",
        "    path=\"/generative_ai\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQtC1QBNoJg3",
        "outputId": "61cc17f1-9dd7-46cc-dab2-d336a4513b0c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/src/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_langchain/requirements.txt\n",
        "torch==2.2.2\n",
        "transformers==4.39.3\n",
        "accelerate==0.28.0\n",
        "bitsandbytes==0.42.0\n",
        "huggingface-hub==0.22.2\n",
        "langchain==0.1.14\n",
        "langchain-core==0.1.43\n",
        "langchain-community==0.0.31\n",
        "pypdf==4.2.0\n",
        "sentence-transformers==2.6.1\n",
        "beautifulsoup4==4.12.3\n",
        "langserve[all]\n",
        "chromadb==0.4.24\n",
        "langchain-chroma==0.1.0\n",
        "faiss-cpu==1.8.0\n",
        "unstructured==0.13.2\n",
        "fastapi==0.110.1\n",
        "uvicorn==0.29.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKgNYrajoNCN",
        "outputId": "c99bb207-e37f-44a8-9c07-51e849f27153"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_langchain/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL2kqwcFqlf9",
        "outputId": "ffad0420-8a6d-4e6c-82f3-d7e231fc44f3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Thêm authtoken vào ngrok\n",
        "ngrok.set_auth_token(\"2w4qyWEnLQ7JNLh8Ck2cS6Nu927_hNKdLKSL1tSSaGR6VjZY\")\n",
        "\n",
        "# Kiểm tra kết nối\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"API is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4tjTL3rgnf",
        "outputId": "3f372a08-3486-4a72-a97b-dd3d367875ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API is live at: NgrokTunnel: \"https://59b2-34-124-176-98.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# Khởi động server uvicorn trên cổng 5000\n",
        "subprocess.run([\"uvicorn\", \"src.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"5000\", \"--reload\"])\n"
      ],
      "metadata": {
        "id": "sytvdboP41TG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}