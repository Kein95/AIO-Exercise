# -*- coding: utf-8 -*-
"""M10-W1-Pre-training-GPT-Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lkix0nzEv_kX8-npoGbf7ZgkjkafBtWC
"""

!pip install datasets

from datasets import load_dataset

ds = load_dataset("datablations/c4-filter-small", split="train")     # Dòng 3
ds = ds.select_columns(["text"])                                     # Dòng 4
ds = ds.train_test_split(test_size=0.1)                              # Dòng 5

!pip install tokenizers

# 1. Tải bộ dữ liệu C4-filter nhỏ
from datasets import load_dataset
ds = load_dataset("datablations/c4-filter-small", split="train")
ds = ds.select_columns(["text"])
ds = ds.train_test_split(test_size=0.1)

# 2. Import các thành phần của Tokenizer
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.normalizers import NFKC
from tokenizers.decoders import ByteLevel as ByteLevelDecoder

# 3. Khởi tạo tokenizer
tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = ByteLevel()
tokenizer.normalizer = NFKC()
tokenizer.decoder = ByteLevelDecoder()

# 4. Cấu hình trainer với vocab_size = 50257 (chuẩn GPT-2)
trainer = BpeTrainer(
    vocab_size=50257,
    special_tokens=["<s>", "<pad>", "</s>", "<unk>", "<mask>"]
)

# 5. Huấn luyện tokenizer từ text
tokenizer.train_from_iterator(ds["train"]["text"], trainer)

# 6. Lưu tokenizer
tokenizer.save("gpt_tokenizer.json")

# 7. (Tuỳ chọn) Kiểm tra tokenizer mới huấn luyện
output = tokenizer.encode("I am learning to pretrain GPT2 from scratch.")
print("Token IDs:", output.ids)
print("Tokens:", output.tokens)

def tokenize(example):
    encoded_batch = tokenizer.encode_batch(example["text"])
    return {
        "input_ids": [e.ids for e in encoded_batch],
        "attention_mask": [e.attention_mask for e in encoded_batch]
    }

tokenized_ds = ds.map(
    tokenize,
    remove_columns=["text"],
    batched=True,
    num_proc=20
)

block_size = 512

def group_texts(examples):
    # concat input_ids
    concatenated = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated["input_ids"])
    total_length = (total_length // block_size) * block_size

    # split block_size
    result = {
        k: [concatenated[k][i : i + block_size] for i in range(0, total_length, block_size)]
        for k in concatenated
    }

    # prepare labels
    result["labels"] = result["input_ids"].copy()
    return result

lm_ds = tokenized_ds.map(group_texts, batched=True, num_proc=20)