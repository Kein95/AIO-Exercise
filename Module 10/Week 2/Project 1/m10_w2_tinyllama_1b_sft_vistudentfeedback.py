# -*- coding: utf-8 -*-
"""M10-W2_TinyLlama_1B_SFT_ViStudentFeedback.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X4Tic-mcLsiRLPNoMbB4qgLh_QK9mfm3

## **1. Install and import necessary libaries**
"""

!pip install -q -U bitsandbytes
!pip install -q -U datasets
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U loralib
!pip install -q -U einops

import os
import torch
import numpy as np
from sklearn.metrics import accuracy_score
from datasets import load_dataset

from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback,
    DataCollatorWithPadding,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig,
    set_seed
)

from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    PeftModel,
    PeftConfig
)

set_seed(42)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

"""## **2. Load & Preprocess Dataset**"""

dataset = load_dataset("uit-nlp/vietnamese_students_feedback", trust_remote_code=True)

print("Dataset structure:", dataset)

def preprocess(example):
    return {
        "text": str(example["sentence"]),
        "label": int(example["sentiment"])
    }

dataset = dataset.map(
    preprocess,
    remove_columns=["sentence", "sentiment", "topic"]
)

dataset

"""## **3. Load Pre-trained Model**"""

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=3,
    quantization_config=bnb_config,
    id2label={0: "tiêu_cực", 1: "trung_lập", 2: "tích_cực"},
    label2id={"tiêu_cực": 0, "trung_lập": 1, "tích_cực": 2},
    pad_token_id=tokenizer.pad_token_id
)

model = prepare_model_for_kbit_training(model)
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_CLS",
    inference_mode=False
)
model = get_peft_model(model, peft_config)

model.print_trainable_parameters()

"""### **3.1 Tokenization**"""

def tokenize_function(examples):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        max_length=256,
        padding="max_length",
        add_special_tokens=True
    )
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": examples["label"]
    }

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text", "label"]
)

print("\nDataset sau khi xử lý:")
print(tokenized_dataset)

print("\nVí dụ 1 mẫu train:")
print(tokenized_dataset["train"][0])

"""### **3.2 Test pre-trained model**"""

def test_output_before_training(model, tokenizer, samples=None):
    if samples is None:
        samples = [
            "Giảng viên giảng bài dễ hiểu và nhiệt tình.",
            "Em cảm thấy rất áp lực với lịch học dày đặc.",
            "Bình thường, không có gì đặc biệt lắm."
        ]

    model.eval()
    print("==> Test kết quả dự đoán (model chưa được fine-tuned):\n")
    for i, text in enumerate(samples, 1):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256, padding=True).to(model.device)
        with torch.no_grad():
            outputs = model(**inputs)
            pred_class = torch.argmax(outputs.logits, dim=-1).item()
            pred_label = model.config.id2label[pred_class]
        print(f"{i}. \"{text}\"\n→ Dự đoán: {pred_label} (label id: {pred_class})\n")

test_output_before_training(model, tokenizer)

"""## **4. Training**

### **4.1 Config & Train**
"""

data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8,
    return_tensors="pt"
)

def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {"accuracy": accuracy_score(labels, preds)}

class LossLoggerCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            print(f"[Step {state.global_step:>5}] Loss: {logs['loss']:.4f}")

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=1,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True,
    gradient_checkpointing=True,
    remove_unused_columns=True,
    logging_dir="./logs",
    logging_steps=50,
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none",
    optim="paged_adamw_8bit",
    label_names=["labels"]
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    callbacks=[LossLoggerCallback()]
)

trainer.train()

torch.cuda.empty_cache()

"""### **4.2 Evaluate on Test**"""

test_dataset = tokenized_dataset["test"]
print("→ Test set:", len(test_dataset), "samples")

test_results = trainer.evaluate(eval_dataset=test_dataset)
print("\nEvaluate on test set:")
print({k: round(v, 4) for k, v in test_results.items()})

test_samples = [
    "Giảng viên giảng bài rất dễ hiểu, em cảm thấy hứng thú với môn học.",
    "Lịch học quá dày, em cảm thấy rất mệt mỏi và không tiếp thu được bài.",
    "Em thấy môn học này khá ổn, không có gì đặc biệt.",
    "Thầy cô tận tình, tuy nhiên hệ thống lớp học trực tuyến hay bị lỗi.",
    "Môn này khó quá, em học mãi không hiểu gì hết.",
    "Quá tuyệt vời, không thể nào tốt hơn"
]

print("\nKết quả phân loại cảm xúc:")
model.eval()
for i, text in enumerate(test_samples, 1):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256).to(model.device)
    with torch.no_grad():
        outputs = model(**inputs)
    predicted_class = torch.argmax(outputs.logits).item()
    predicted_label = model.config.id2label[predicted_class]
    print(f"{i}. {text}\n→ {predicted_label}\n")

"""## **5. Save fine-tuned model**"""

model_save_path = "./fine_tuned_model"
model.save_pretrained(model_save_path, safe_serialization=True)
tokenizer.save_pretrained(model_save_path)

"""## **6. Inference**"""

model_save_path = "./fine_tuned_model"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

base_model = AutoModelForSequenceClassification.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    quantization_config=bnb_config,
    num_labels=3,
    id2label={0: "tiêu_cực", 1: "trung_lập", 2: "tích_cực"},
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, model_save_path)
model = model.merge_and_unload()

tokenizer = AutoTokenizer.from_pretrained(model_save_path)

def predict_sentiment(text, model, tokenizer):
    inputs = tokenizer(
        text,
        truncation=True,
        max_length=256,
        padding=True,
        return_tensors="pt"
    ).to(model.device)

    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)

    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_class = torch.argmax(probabilities).item()
    predicted_label = model.config.id2label[predicted_class]
    confidence = probabilities[0][predicted_class].item()

    return {
        "text": text,
        "label": predicted_label,
        "confidence": round(confidence, 4),
        "probabilities": {
            "tiêu_cực": round(probabilities[0][0].item(), 4),
            "trung_lập": round(probabilities[0][1].item(), 4),
            "tích_cực": round(probabilities[0][2].item(), 4)
        }
    }

test_samples = [
    "Giáo viên dạy rất nhiệt tình và dễ hiểu",
    "Phòng học quá chật hẹp và thiếu trang thiết bị",
    "Môn học này khá thú vị nhưng lượng bài tập hơi nhiều",
    "Em thấy bình thường",
    "Giảng viên nói về môn học này, nghe như đang kể chuyện cổ tích, khó tin nhưng thú vị.",
    "Cơ sở vật chất tuyệt vời, chỉ thiếu mỗi việc có thể đưa tôi lên mặt trăng.",
    "Môn học này, tôi chắc chắn là mình đã học, nhưng liệu có thể nhớ được không thì chưa biết."
]


for sample in test_samples:
    result = predict_sentiment(sample, model, tokenizer)
    print(f"→ Văn bản: {result['text']}")
    print(f"   Nhãn dự đoán: {result['label']} (Độ tin cậy: {result['confidence']*100:.2f}%)")
    print(f"   Phân phối xác suất: {result['probabilities']}\n")